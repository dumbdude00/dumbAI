{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8d84e9d-7035-456a-ba90-53e6eb50306d",
   "metadata": {},
   "source": [
    "## This nb borrowed lines from fastai, OpenAI & ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff34ba2e-329b-417e-a8e4-c32077a93aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34d8eba6-0cb1-4b2b-a28c-e80424012298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF,torch.nn.functional as F\n",
    "from miniai.imports import *\n",
    "from miniai.datasets import *\n",
    "from datasets import load_dataset,load_dataset_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b224b5-946e-4a51-91de-6ae5b786349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d59c0c18-47a2-478b-b15c-77eda169fd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of GPUs available =  1\n"
     ]
    }
   ],
   "source": [
    "class GPUCUDAMissing(BaseException):\n",
    "    pass\n",
    "    \n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        print('# of GPUs available = ', torch.cuda.device_count())\n",
    "    else:\n",
    "        raise GPUCUDAMissing\n",
    "except GPUCUDAMissing:\n",
    "    print(\"ERROR: GPU is missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fdc6c4-776d-4259-ab47-5a7009bec5dc",
   "metadata": {},
   "source": [
    "### Loading Tiny-Imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a5021c4-9df1-4793-8e55-7151982ec4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xl,yl = 'image','label'\n",
    "name = \"zh-plus/tiny-imagenet\"\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49283a91-03fc-47a1-ad4d-156d6df8ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[xl] = [(torch.ones([3,1,1])*(TF.to_tensor(o)-0.0)) for o in b[xl]]\n",
    "\n",
    "bs = 32\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13eeddce-48b5-491d-adc7-31da319fe9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6300aae2-b5e8-46d7-b704-931c4c6513b7",
   "metadata": {},
   "source": [
    "### Generate captions for CLIP to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06f566bd-e6df-4e55-8c0f-e257c8b21e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the files\n",
    "fp_json_pos2idx = hf_hub_download(repo_id=name, filename=\"dataset_infos.json\", repo_type=\"dataset\")\n",
    "fp_idx2human = hf_hub_download(repo_id=name, filename=\"classes.py\", repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4155fa3d-af77-445c-8c9e-5b27a3161a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(fp_json_pos2idx,) \n",
    "data_pos2idx = json.load(f) \n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bd41d7f-d5e8-4c8a-b60c-9a788ae539ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repo provided a variable called 'i2d'\n",
    "exec(open(fp_idx2human).read()) \n",
    "# Let's rename this variable to something suitable for this notebook\n",
    "idx2human = i2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3373a79-fc63-41ab-9dd2-28c360340292",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos2idx = data_pos2idx['Maysee--tiny-imagenet']['features']['label']['names']\n",
    "pos2human = [idx2human[v] for k,v in enumerate(pos2idx)]\n",
    "captions = [\"A photo of a \" + txt.split(\",\")[0] for txt in pos2human]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb141f5b-de65-436a-8bc4-5a092f3e8646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clipmodel = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "#processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cb1c6fe-9030-4ba5-bd99-52f9003c7e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fastai_zaki_env/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "clipmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c295b19-c841-4fa6-ac5a-0269c9f5a68c",
   "metadata": {},
   "source": [
    "Optional: Use the cmd below to determine if VM-Container is paging while loading these models\n",
    "\n",
    "apt update && apt-get install sysstat && pidstat -r -d --human 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "704c92e2-baf1-4574-bca0-5f8b6a2dfe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_images(xb[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d664887-222f-4bfa-9391-5fc9595440ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 64, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c25a8-e0ff-4d8f-9911-8becd5f8c3ff",
   "metadata": {},
   "source": [
    "### CLIP: (img, text) -> (img_emb, text_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e93ca97-f0a1-473e-9cfd-99ffb0b731fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=captions, images=xb, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "image_input = inputs[\"pixel_values\"].to(\"cuda\")\n",
    "text_inputs = inputs[\"input_ids\"].to(\"cuda\")\n",
    "attention_mask = inputs[\"attention_mask\"].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92123b0e-b3bf-4a5e-9bb9-22aad45126ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Compute the image and text embeddings\n",
    "    image_features = clipmodel.get_image_features(image_input)\n",
    "    text_features = clipmodel.get_text_features(input_ids=text_inputs, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8657517a-4200-4e6e-811a-56b73fb56958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 512]), torch.Size([200, 512]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape, text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "363db431-7f3e-40ff-a39c-c8fe658a79d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd704cac-660d-49cd-9d50-e8f6f89197fd",
   "metadata": {},
   "source": [
    "### Pass img_features to SD training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8b5a704-5fd0-414f-872c-f0143307c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def abar(t): return (t*math.pi/2).cos()**2\n",
    "def inv_abar(x): return x.sqrt().acos()*2/math.pi\n",
    "\n",
    "def noisify(x0):\n",
    "    device = x0.device\n",
    "    n = len(x0)\n",
    "    t = torch.rand(n,).to(x0).clamp(0,0.999)\n",
    "    ε = torch.randn(x0.shape, device=device)\n",
    "    abar_t = abar(t).reshape(-1, 1, 1, 1).to(device)\n",
    "    xt = abar_t.sqrt()*x0 + (1-abar_t).sqrt()*ε\n",
    "    return (xt, t.to(device)), ε\n",
    "\n",
    "def collate_ddpm(b): return noisify(default_collate(b)[xl])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6981aff3-7eee-4a79-bb91-5b491998ea8b",
   "metadata": {},
   "source": [
    "Info: In order to fix some CUDA multiprocessing issue which I don't understand. Please set num_workers=0 as shown in the cell below. Source: https://github.com/pytorch/pytorch/issues/40403#issuecomment-731782611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e31c80df-ce8c-47c8-ad65-988e7b074430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_ddpm(ds): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53153150-4670-4797-ae8b-8efdf7cfc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_ddpm(b):\n",
    "    #import pdb; pdb.set_trace()    \n",
    "    b = default_collate(b)\n",
    "    #(xt,t),eps = noisify(b[xl])       # original line from Jeremy\n",
    "    \n",
    "    # ok, let's do the padding and shifting the range from (0,1) to (-0.5,0.5) for SD\n",
    "    b_padded_n_shifted = F.pad(b[xl], (0,0,0,0))-0.5\n",
    "    (xt,t),eps = noisify(b_padded_n_shifted)\n",
    "    \n",
    "    # Below are lines for CLIP\n",
    "    inputs = processor(images=b[xl], return_tensors=\"pt\", padding=True, do_rescale=False).to(\"cuda\")\n",
    "    image_input = inputs[\"pixel_values\"]\n",
    "    with torch.no_grad():\n",
    "        image_features = clipmodel.get_image_features(image_input)\n",
    "    return (xt,t,image_features),eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98c18232-8c90-486f-a1bb-5cad881dd68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[xl] = [ (torch.ones([3,1,1])*TF.to_tensor(o)) for o in b[xl]] # some images have 1 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0f8eda8-dd11-4b97-a031-0e9e96cba045",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['valid']))\n",
    "\n",
    "dl = dls.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc422cb4-24cf-4149-b2b6-c7b3902f4dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.96 s, sys: 423 ms, total: 4.38 s\n",
      "Wall time: 164 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(xt,t,image_features),eps = b = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56dbc237-0007-40cd-8c82-92170420e7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 64, 64]),\n",
       " torch.Size([32]),\n",
       " torch.Size([32, 512]),\n",
       " torch.Size([32, 3, 64, 64]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt.shape, t.shape, image_features.shape, eps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c4b30-ecd6-4680-8aa8-c71f1809e840",
   "metadata": {},
   "source": [
    "### Build Conditional UNet from Lecture 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b3cbe-53a4-47a2-8288-779ba654a350",
   "metadata": {},
   "source": [
    "# Diffusion unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "553b2ef1-f41c-4d49-ad92-42b582f4d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d806531-5ac6-4ff9-bb14-2376ac7ed3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from miniai.imports import *\n",
    "\n",
    "from einops import rearrange\n",
    "from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d68bcb1e-47fa-4082-b85c-92eb28ae4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray_r'\n",
    "mpl.rcParams['figure.dpi'] = 70\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "set_seed(42)\n",
    "if fc.defaults.cpus>8: fc.defaults.cpus=7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f30c6c6-c5ba-454e-b970-fe373d051de8",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b77ce0a-15b6-49e2-94ed-938530532a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def timestep_embedding(tsteps, emb_dim, max_period= 10000):\n",
    "    exponent = -math.log(max_period) * torch.linspace(0, 1, emb_dim//2, device=tsteps.device)\n",
    "    emb = tsteps[:,None].float() * exponent.exp()[None,:]\n",
    "    emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "    return F.pad(emb, (0,1,0,0)) if emb_dim%2==1 else emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d28ba08-48f6-4a5e-b023-16119bf709d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pre_conv(ni, nf, ks=3, stride=1, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca7f565e-aeea-498d-a592-7859e930dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def upsample(nf): return nn.Sequential(nn.Upsample(scale_factor=2.), nn.Conv2d(nf, nf, 3, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f1e5204-011f-47f7-a467-410c218a688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lin(ni, nf, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Linear(ni, nf, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "beb788d0-26f0-40ca-a0ac-e1ae8591ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version is giving poor results - use the cell below instead\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni, attn_chans):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(ni, ni//attn_chans, batch_first=True)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n,c,h,w = x.shape\n",
    "        x = self.norm(x).view(n, c, -1).transpose(1, 2)\n",
    "        x = self.attn(x, x, x, need_weights=False)[0]\n",
    "        return x.transpose(1,2).reshape(n,c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1318b9b6-d15c-4e70-8374-c33bb68e49d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni, attn_chans, transpose=True):\n",
    "        super().__init__()\n",
    "        self.nheads = ni//attn_chans\n",
    "        self.scale = math.sqrt(ni/self.nheads)\n",
    "        self.norm = nn.LayerNorm(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "        self.t = transpose\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n,c,s = x.shape\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        x = self.qkv(x)\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)\n",
    "        q,k,v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        x = self.proj(x)\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6d02767-27f2-4a91-b1ef-39261d3875fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SelfAttention2D(SelfAttention):\n",
    "    def forward(self, x):\n",
    "        n,c,h,w = x.shape\n",
    "        return super().forward(x.view(n, c, -1)).reshape(n,c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05b9f758-acf8-4470-839c-d8bc044f325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EmbResBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, nf=None, ks=3, act=nn.SiLU, norm=nn.BatchNorm2d, attn_chans=0):\n",
    "        super().__init__()\n",
    "        if nf is None: nf = ni\n",
    "        self.emb_proj = nn.Linear(n_emb, nf*2)\n",
    "        self.conv1 = pre_conv(ni, nf, ks, act=act, norm=norm)\n",
    "        self.conv2 = pre_conv(nf, nf, ks, act=act, norm=norm)\n",
    "        self.idconv = fc.noop if ni==nf else nn.Conv2d(ni, nf, 1)\n",
    "        self.attn = False\n",
    "        if attn_chans: self.attn = SelfAttention2D(nf, attn_chans)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        inp = x\n",
    "        x = self.conv1(x)\n",
    "        emb = self.emb_proj(F.silu(t))[:, :, None, None]\n",
    "        scale,shift = torch.chunk(emb, 2, dim=1)\n",
    "        x = x*(1+scale) + shift\n",
    "        x = self.conv2(x)\n",
    "        x = x + self.idconv(inp)\n",
    "        if self.attn: x = x + self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7dfd646a-e21d-438f-8bf8-551217dcefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModule:\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        self.saved = super().forward(x, *args, **kwargs)\n",
    "        return self.saved\n",
    "\n",
    "class SavedEmbResBlock(SaveModule, EmbResBlock): pass\n",
    "class SavedConv2d(SaveModule, nn.Conv2d): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ff112c2-cf5e-4a83-aadf-0f9ca9ca2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, nf, add_down=True, num_layers=1, attn_chans=0):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList([SavedEmbResBlock(n_emb, ni if i==0 else nf, nf, attn_chans=attn_chans)\n",
    "                                      for i in range(num_layers)])\n",
    "        self.down = SavedConv2d(nf, nf, 3, stride=2, padding=1) if add_down else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.saved = []\n",
    "        for resnet in self.resnets: \n",
    "            x = resnet(x, t)\n",
    "            self.saved.append(resnet.saved)\n",
    "        x = self.down(x)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        if isinstance(self.down, SavedConv2d): self.saved.append(self.down.saved)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e07e045-9c1f-4691-921c-ea1427a63433",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, prev_nf, nf, add_up=True, num_layers=2, attn_chans=0):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList(\n",
    "            [EmbResBlock(n_emb, (prev_nf if i==0 else nf)+(ni if (i==num_layers-1) else nf), nf, attn_chans=attn_chans)\n",
    "            for i in range(num_layers)])\n",
    "        self.up = upsample(nf) if add_up else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t, ups):\n",
    "        for resnet in self.resnets: x = resnet(torch.cat([x, ups.pop()], dim=1), t)\n",
    "        return self.up(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b891bdd-237d-4374-967b-e8adcd8e1163",
   "metadata": {},
   "source": [
    "## Conditional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa431ed7-051b-4878-b393-e553b8dcf6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondUNetModel(nn.Module):\n",
    "    def __init__( self, n_classes, clip_emd_size, in_channels=3, out_channels=3, nfs=(224,448,672,896), num_layers=1):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)\n",
    "        self.n_temb = nf = nfs[0]\n",
    "        n_emb = nf*4\n",
    "        self.cond_emb = nn.Sequential(lin(clip_emd_size, n_emb, norm=nn.BatchNorm1d),\n",
    "                                     lin(n_emb, n_emb))\n",
    "        self.emb_mlp = nn.Sequential(lin(self.n_temb, n_emb, norm=nn.BatchNorm1d),\n",
    "                                     lin(n_emb, n_emb))\n",
    "        self.downs = nn.ModuleList()\n",
    "        for i in range(len(nfs)):\n",
    "            ni = nf\n",
    "            nf = nfs[i]\n",
    "            self.downs.append(DownBlock(n_emb, ni, nf, add_down=i!=len(nfs)-1, num_layers=num_layers))\n",
    "        self.mid_block = EmbResBlock(n_emb, nfs[-1])\n",
    "\n",
    "        rev_nfs = list(reversed(nfs))\n",
    "        nf = rev_nfs[0]\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i in range(len(nfs)):\n",
    "            prev_nf = nf\n",
    "            nf = rev_nfs[i]\n",
    "            ni = rev_nfs[min(i+1, len(nfs)-1)]\n",
    "            self.ups.append(UpBlock(n_emb, ni, prev_nf, nf, add_up=i!=len(nfs)-1, num_layers=num_layers+1))\n",
    "        self.conv_out = pre_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x,t,c = inp\n",
    "        temb = timestep_embedding(t, self.n_temb)\n",
    "        cemb = self.cond_emb(c)\n",
    "        emb = self.emb_mlp(temb) + cemb\n",
    "        x = self.conv_in(x)\n",
    "        saved = [x]\n",
    "        for block in self.downs: x = block(x, emb)\n",
    "        #import pdb; pdb.set_trace() \n",
    "        saved += [p for o in self.downs for p in o.saved]\n",
    "        x = self.mid_block(x, emb)\n",
    "        for block in self.ups: x = block(x, emb, saved)\n",
    "        return self.conv_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3656ae8d-e491-4f78-b803-d671be4216c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "epochs = 25\n",
    "opt_func = partial(optim.Adam, eps=1e-5)\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\n",
    "model = CondUNetModel(n_classes=200, clip_emd_size=512, in_channels=3, out_channels=3, nfs=(32,64,128,256), num_layers=2)\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae67c4-bd4a-4ca4-b5c9-e9d841197157",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b555b-172e-4ba8-b3d5-0a86e4d49e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'sd_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb28bf-e5ab-4100-9556-4b1e2116a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lded_model = torch.load('sd_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8972039-df7d-4d96-a3d3-bc49f7fe84ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(learn, 'sd_learn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6139d731-f412-419c-b295-4449970f80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lded_learn = torch.load('sd_learn.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast.ai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
